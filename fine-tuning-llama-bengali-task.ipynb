{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/samardas/fine-tuning-llama-bengali-task?scriptVersionId=290221860\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Import libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers peft datasets evaluate sentencepiece safetensors rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T17:29:10.418029Z","iopub.execute_input":"2026-01-05T17:29:10.418252Z","iopub.status.idle":"2026-01-05T17:29:33.066015Z","shell.execute_reply.started":"2026-01-05T17:29:10.418219Z","shell.execute_reply":"2026-01-05T17:29:33.065112Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Fine-tune LLaMA 3.1-8B-Instruct on Bengali Empathetic Data","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Fine-tune LLaMA 3.1-8B-Instruct on Bengali Empathetic Data\n# LoRA + 4-bit (bitsandbytes) – OOM-aware on T4/Colab\n#\n# Requirements covered:\n# - LoRA (via PEFT) on LLaMA 3.1-8B\n# - 4-bit quantization (QLoRA style)\n# - Full max_seq_len = 512 with dynamic padding\n# - OOP: DatasetProcessor, LLAMAFineTuner, Evaluator\n# - Strategy pattern: LoRAStrategy (+ UnslothStrategy stub)\n# - Metrics: Perplexity, BLEU, ROUGE\n# - SQLite logging: LLAMAExperiments, GeneratedResponses\n# - Human eval export file\n# ============================================================\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:256\"\n\nimport gc\nimport math\nimport json\nimport sqlite3\nfrom datetime import datetime\n\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n)\nimport evaluate\n\n# ============================================================\n# 0. GLOBAL CONFIG & UTILITIES\n# ============================================================\n\nMODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\nMAX_SEQ_LEN = 512\nDB_PATH = \"experiments.db\"\n\n# HF token: set via environment or replace string below for local testing.\nHF_TOKEN = os.environ.get(\"HF_TOKEN\", \"****\")\n# IMPORTANT: Replace the placeholder above or set HF_TOKEN in your env.\n# Do NOT commit your real token.\n\ndef clear_memory():\n    \"\"\"Free Python and CUDA memory to reduce fragmentation / OOM risk.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nclear_memory()\n\n# ============================================================\n# 1. STRATEGY PATTERN: FineTuneStrategy, LoRAStrategy, UnslothStrategy\n# ============================================================\n\nclass FineTuneStrategy:\n    \"\"\"\n    Abstract base class for fine-tuning strategies (LoRA vs Unsloth).\n    \"\"\"\n    def setup_model(self, model_name: str, hf_token: str, max_seq_len: int):\n        \"\"\"\n        Should return: (model, tokenizer, lora_config_dict)\n        \"\"\"\n        raise NotImplementedError\n\n\nclass LoRAStrategy(FineTuneStrategy):\n    \"\"\"\n    Standard Hugging Face + PEFT + bitsandbytes LoRA (QLoRA style).\n    \"\"\"\n\n    def __init__(self, r=8, lora_alpha=16, lora_dropout=0.05):\n        self.r = r\n        self.lora_alpha = lora_alpha\n        self.lora_dropout = lora_dropout\n\n    def setup_model(self, model_name: str, hf_token: str, max_seq_len: int):\n        # 1) Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            token=hf_token,\n            use_fast=False,\n        )\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n\n        # 2) BitsAndBytes 4-bit config\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16,\n        )\n\n        # 3) Load 4-bit base model\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            token=hf_token,\n            quantization_config=bnb_config,\n            device_map=\"auto\",           # let HF decide placement\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True,\n        )\n\n        # 4) Prepare for k-bit training + gradient checkpointing\n        model.gradient_checkpointing_enable()\n        model = prepare_model_for_kbit_training(model)\n\n        # 5) LoRA config (attention-only for efficiency)\n        lora_config = LoraConfig(\n            r=self.r,\n            lora_alpha=self.lora_alpha,\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n            lora_dropout=self.lora_dropout,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n\n        # 6) Apply LoRA\n        model = get_peft_model(model, lora_config)\n        model.config.use_cache = False\n        model.print_trainable_parameters()\n\n        lora_config_dict = {\n            \"type\": \"LoRA-PEFT\",\n            \"r\": self.r,\n            \"lora_alpha\": self.lora_alpha,\n            \"lora_dropout\": self.lora_dropout,\n            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        }\n\n        return model, tokenizer, lora_config_dict\n\n\nclass UnslothStrategy(FineTuneStrategy):\n    \"\"\"\n    Stub for Unsloth-based strategy.\n    You can implement this with unsloth.FastLanguageModel if you install Unsloth.\n    For the assignment, this class + LoRAStrategy demonstrate the Strategy pattern.\n    \"\"\"\n\n    def setup_model(self, model_name: str, hf_token: str, max_seq_len: int):\n        raise NotImplementedError(\n            \"UnslothStrategy not implemented in this script. \"\n            \"Implement with unsloth.FastLanguageModel if desired.\"\n        )\n\n# ============================================================\n# 2. DATASET PROCESSOR\n# ============================================================\n\nclass DatasetProcessor:\n    \"\"\"\n    Handles:\n    - Building LLaMA chat-style prompts from input/response pairs.\n    - Tokenization with full sequence length and dynamic padding.\n    \"\"\"\n\n    def __init__(self, tokenizer, max_seq_len: int):\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def build_prompt(self, example):\n        \"\"\"\n        Build LLaMA 3.1 chat-style prompt.\n\n        Structure:\n          <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n          {input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n          {response}<|eot_id|>\n        \"\"\"\n        return (\n            \"<|begin_of_text|>\"\n            \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n            f\"{example['input']}<|eot_id|>\"\n            \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n            f\"{example['response']}<|eot_id|>\"\n        )\n\n    def tokenize(self, example):\n        \"\"\"\n        Tokenize a single example:\n        - Truncate at max_seq_len (512)\n        - No padding here (dynamic padding in data collator)\n        - labels = input_ids for causal LM\n        \"\"\"\n        text = self.build_prompt(example)\n        enc = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=self.max_seq_len,\n            padding=False,\n        )\n        enc[\"labels\"] = enc[\"input_ids\"].copy()\n        return enc\n\n    def prepare(self, data_list, val_size=0.2):\n        \"\"\"\n        Convert a Python list of dicts to HF Dataset,\n        split into train/val, and tokenize.\n\n        data_list: list of {\"input\": ..., \"response\": ...}\n        returns: (train_tokenized, val_tokenized, val_raw_for_eval)\n        \"\"\"\n        ds = Dataset.from_list(data_list).train_test_split(\n            test_size=val_size,\n            seed=42,\n        )\n        train_raw = ds[\"train\"]\n        val_raw = ds[\"test\"]\n\n        train_tokenized = train_raw.map(\n            self.tokenize,\n            remove_columns=train_raw.column_names,\n        )\n        val_tokenized = val_raw.map(\n            self.tokenize,\n            remove_columns=val_raw.column_names,\n        )\n\n        return train_tokenized, val_tokenized, val_raw\n\n# ============================================================\n# 3. LLAMAFineTuner\n# ============================================================\n\nclass LLAMAFineTuner:\n    \"\"\"\n    Orchestrates:\n    - Model + tokenizer setup via a FineTuneStrategy (LoRA or Unsloth)\n    - Trainer creation\n    \"\"\"\n\n    def __init__(self, strategy: FineTuneStrategy, model_name: str, hf_token: str, max_seq_len: int):\n        self.strategy = strategy\n        self.model_name = model_name\n        self.hf_token = hf_token\n        self.max_seq_len = max_seq_len\n\n        # Use the chosen strategy to get model + tokenizer + lora_config dict\n        self.model, self.tokenizer, self.lora_config = self.strategy.setup_model(\n            model_name=self.model_name,\n            hf_token=self.hf_token,\n            max_seq_len=self.max_seq_len,\n        )\n\n    def get_trainer(self, train_ds, val_ds, output_dir=\"./llama31_bn_lora_outputs\"):\n        \"\"\"\n        Build a Hugging Face Trainer for causal LM fine-tuning.\n        \"\"\"\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.tokenizer,\n            mlm=False,\n        )\n\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            per_device_train_batch_size=1,\n            per_device_eval_batch_size=1,\n            gradient_accumulation_steps=4,\n            num_train_epochs=3,\n            learning_rate=2e-4,\n            fp16=True,\n            logging_steps=5,\n            save_steps=1000,\n            save_total_limit=1,\n            report_to=\"none\",\n        )\n\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=train_ds,\n            eval_dataset=val_ds,\n            data_collator=data_collator,\n        )\n        return trainer\n\n# ============================================================\n# 4. EVALUATOR (perplexity, BLEU, ROUGE, human eval export)\n# ============================================================\n\nclass Evaluator:\n    \"\"\"\n    Handles:\n    - Perplexity computation via Trainer\n    - Text generation\n    - BLEU & ROUGE\n    - Export of samples for human evaluation\n    \"\"\"\n\n    def __init__(self, model, tokenizer, processor: DatasetProcessor, trainer: Trainer, max_seq_len: int):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.processor = processor\n        self.trainer = trainer\n        self.max_seq_len = max_seq_len\n        self.device = next(self.model.parameters()).device\n\n        self.bleu_metric = evaluate.load(\"bleu\")\n        self.rouge_metric = evaluate.load(\"rouge\")\n\n    def compute_perplexity(self, eval_dataset):\n        \"\"\"\n        Compute perplexity from eval loss.\n        \"\"\"\n        results = self.trainer.evaluate(eval_dataset=eval_dataset)\n        eval_loss = results[\"eval_loss\"]\n        ppl = math.exp(eval_loss)\n        return ppl, eval_loss\n\n    def generate_response(self, user_text: str, max_new_tokens: int = 128):\n        \"\"\"\n        Generate a response for a given user_text using chat-style prompt.\n        \"\"\"\n        prompt = (\n            \"<|begin_of_text|>\"\n            \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n            f\"{user_text}<|eot_id|>\"\n            \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        )\n\n        inputs = self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=self.max_seq_len,\n        ).to(self.device)\n\n        self.model.eval()\n        with torch.no_grad():\n            output_ids = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n                top_k=50,\n                repetition_penalty=1.2,\n                no_repeat_ngram_size=3,\n                pad_token_id=self.tokenizer.eos_token_id,\n            )\n\n        full_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        return full_text\n\n    def compute_bleu_rouge(self, val_raw, num_samples: int = None, log_callback=None):\n        \"\"\"\n        Compute BLEU & ROUGE on a subset of val_raw.\n\n        val_raw: HF Dataset with columns [\"input\", \"response\"]\n        num_samples: limit number of samples (None = use all)\n        log_callback: optional function(input_text, pred_text) for logging\n        \"\"\"\n        if num_samples is None:\n            n = len(val_raw)\n        else:\n            n = min(num_samples, len(val_raw))\n\n        inputs = []\n        preds = []\n        refs = []\n\n        for ex in val_raw.select(range(n)):\n            user_text = ex[\"input\"]\n            ref = ex[\"response\"]\n            pred = self.generate_response(user_text)\n\n            inputs.append(user_text)\n            preds.append(pred)\n            refs.append(ref)\n\n            if log_callback is not None:\n                log_callback(user_text, pred)\n\n        bleu = self.bleu_metric.compute(\n            predictions=preds,\n            references=[[r] for r in refs],\n        )\n        rouge = self.rouge_metric.compute(\n            predictions=preds,\n            references=refs,\n        )\n\n        metrics = {\n            \"bleu\": bleu[\"bleu\"],\n            \"rouge1\": rouge[\"rouge1\"],\n            \"rouge2\": rouge[\"rouge2\"],\n            \"rougeL\": rouge[\"rougeL\"],\n        }\n        return metrics, inputs, preds, refs\n\n    def prepare_human_eval_file(self, inputs, references, predictions, path=\"human_eval_samples.jsonl\"):\n        \"\"\"\n        Export a JSONL file for human evaluation.\n\n        Each line: {\n          \"user_input\": ...,\n          \"reference_response\": ...,\n          \"model_response\": ...\n        }\n        Human raters can add empathy/relevance/fluency scores.\n        \"\"\"\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            for u, r, p in zip(inputs, references, predictions):\n                rec = {\n                    \"user_input\": u,\n                    \"reference_response\": r,\n                    \"model_response\": p,\n                }\n                f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n        return path\n\n# ============================================================\n# 5. SQLITE LOGGER\n# ============================================================\n\nclass ProjectLogger:\n    \"\"\"\n    Logs experiments and generated responses into SQLite.\n\n    Tables:\n      LLAMAExperiments(id, model_name, lora_config, train_loss, val_loss, metrics, timestamp)\n      GeneratedResponses(id, experiment_id, input_text, response_text, timestamp)\n    \"\"\"\n\n    def __init__(self, db_path=DB_PATH):\n        self.db_path = db_path\n        self._init_db()\n\n    def _init_db(self):\n        conn = sqlite3.connect(self.db_path)\n        conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS LLAMAExperiments (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                model_name TEXT,\n                lora_config TEXT,\n                train_loss REAL,\n                val_loss REAL,\n                metrics TEXT,\n                timestamp TEXT\n            )\n            \"\"\"\n        )\n        conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS GeneratedResponses (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                experiment_id INTEGER,\n                input_text TEXT,\n                response_text TEXT,\n                timestamp TEXT\n            )\n            \"\"\"\n        )\n        conn.commit()\n        conn.close()\n\n    def log_experiment(self, model_name, lora_config_dict, train_loss, val_loss, metrics_dict):\n        \"\"\"\n        Insert one experiment row and return experiment_id.\n        \"\"\"\n        conn = sqlite3.connect(self.db_path)\n        cur = conn.cursor()\n        cur.execute(\n            \"\"\"\n            INSERT INTO LLAMAExperiments (model_name, lora_config, train_loss, val_loss, metrics, timestamp)\n            VALUES (?, ?, ?, ?, ?, ?)\n            \"\"\",\n            (\n                model_name,\n                json.dumps(lora_config_dict, ensure_ascii=False),\n                float(train_loss) if train_loss is not None else None,\n                float(val_loss) if val_loss is not None else None,\n                json.dumps(metrics_dict, ensure_ascii=False),\n                datetime.now().isoformat(),\n            ),\n        )\n        experiment_id = cur.lastrowid\n        conn.commit()\n        conn.close()\n        return experiment_id\n\n    def log_response(self, experiment_id: int, input_text: str, response_text: str):\n        \"\"\"\n        Log a single generated response linked to an experiment.\n        \"\"\"\n        conn = sqlite3.connect(self.db_path)\n        conn.execute(\n            \"\"\"\n            INSERT INTO GeneratedResponses (experiment_id, input_text, response_text, timestamp)\n            VALUES (?, ?, ?, ?)\n            \"\"\",\n            (\n                experiment_id,\n                input_text,\n                response_text,\n                datetime.now().isoformat(),\n            ),\n        )\n        conn.commit()\n        conn.close()\n\n# ============================================================\n# 6. BENGALI EMPATHETIC DATA (same as your script)\n# ============================================================\n\nbengali_data = [\n    # Loneliness\n    {\"input\": \"আমার খুব একা লাগছে\", \"response\": \"আমি বুঝতে পারছি, একাকীত্ব অনেক কষ্টের হতে পারে। আমি তোমার পাশে আছি।\"},\n    {\"input\": \"আমার সাথে কেউ কথা বলে না\", \"response\": \"আমি তোমার কথা শুনতে চাই, নিজেকে একা মনে করো না। আমি এখানে আছি তোমার জন্য।\"},\n    {\"input\": \"সবার মাঝে থেকেও নিজেকে খুব একা মনে হয়\", \"response\": \"মাঝে মাঝে ভিড়ের মাঝেও এমন অনুভূতি হতে পারে। তুমি কি কারো সাথে মন খুলে কথা বলতে পেরেছ?\"},\n    # Sadness & depression\n    {\"input\": \"আজ আমার মন খুব খারাপ\", \"response\": \"দুঃখিত শুনে, মন খারাপ হওয়াটা অস্বাভাবিক কিছু না। চাইলে তুমি কারণটি আমার সাথে শেয়ার করতে পারো।\"},\n    {\"input\": \"আমি হতাশ বোধ করছি\", \"response\": \"তুমি একা না, জীবন মাঝে মাঝে কঠিন হয়ে পড়ে। আমি এখানে আছি তোমার কথা শোনার জন্য।\"},\n    {\"input\": \"সবকিছু কেমন জানি অর্থহীন লাগছে\", \"response\": \"হতাশা যখন ঘিরে ধরে তখন এমন মনে হতে পারে। তবে মনে রেখো, তোমার এই অনুভূতিগুলো সাময়িক, আমি তোমার পাশে আছি।\"},\n    {\"input\": \"আমার কান্না পাচ্ছে কিন্তু আমি কাঁদতে পারছি না\", \"response\": \"আবেগ আটকে রাখা খুব কষ্টকর। তুমি যদি মন হালকা করতে চাও, আমি ধৈর্য ধরে শুনছি।\"},\n    # Anxiety & stress\n    {\"input\": \"আমি অনেক চাপের মধ্যে আছি\", \"response\": \"চাপের ফলে এমন লাগা স্বাভাবিক। একটু গভীর নিশ্বাস নাও এবং চেষ্টা করো পর্যাপ্ত বিশ্রাম নেওয়ার।\"},\n    {\"input\": \"ভবিষ্যৎ নিয়ে আমি খুব দুশ্চিন্তায় আছি\", \"response\": \"ভবিষ্যৎ অনিশ্চিত বলে চিন্তা হওয়াটা স্বাভাবিক। তবে ছোট ছোট পদক্ষেপে এগিয়ে যাওয়াটাই এখন সবচেয়ে ভালো হবে।\"},\n    {\"input\": \"সবকিছু কেন আমার সাথে খারাপ হচ্ছে?\", \"response\": \"আমি বুঝতে পারছি তুমি অনেক কষ্টের মধ্য দিয়ে যাচ্ছ। ধৈর্য ধরো, সবকিছু ধীরে ধীরে ঠিক হয়ে যাবে।\"},\n    {\"input\": \"আমার খুব ভয় লাগছে\", \"response\": \"ভয় পাওয়াটা মানবিক অনুভূতি। তুমি কি বলতে পারবে ঠিক কোন জিনিসটা তোমাকে ভাবিয়ে তুলছে?\"},\n    # Failure & confidence\n    {\"input\": \"আমি মনে হয় জীবনে কিছুই করতে পারব না\", \"response\": \"ব্যর্থতা মানেই শেষ নয়, এটি সাফল্যের একটি অংশ। তোমার ভেতরে অনেক সম্ভাবনা আছে।\"},\n    {\"input\": \"সবাই আমাকে ঘৃণা করে\", \"response\": \"তোমার এমন কেন মনে হচ্ছে? তুমি অনেক মূল্যবান একজন মানুষ, নিজেকে ছোট ভেবো না।\"},\n    {\"input\": \"আমি কি কখনো সুখী হতে পারব?\", \"response\": \"অবশ্যই পারবে। সুখের সময়গুলো যেমন ফিরে আসে, দুঃখের সময়গুলোও তেমন চলে যায়। বিশ্বাস হারিও না।\"},\n    # Anger & frustration\n    {\"input\": \"আমি খুব রেগে আছি সবার ওপর\", \"response\": \"রাগ হওয়াটা স্বাভাবিক। চাইলে তুমি রাগটা প্রকাশ করতে পারো, আমি শুনছি। রাগ কমানোর জন্য গভীর শ্বাস নেওয়া খুব কার্যকর হতে পারে।\"},\n    {\"input\": \"কেউ আমাকে বোঝে না\", \"response\": \"না বোঝাটা খুব যন্ত্রণাদায়ক। আমি তোমাকে বোঝার চেষ্টা করছি, তুমি কি বিস্তারিত বলতে চাও?\"},\n    # Social / family\n    {\"input\": \"পরিবারের সাথে আমার কোনো বনিবনা হচ্ছে না\", \"response\": \"পারিবারিক সম্পর্কের জটিলতা অনেক চাপের সৃষ্টি করে। তুমি কি এ নিয়ে কারো সাথে আলাপ করেছ?\"},\n    {\"input\": \"বন্ধুরা আমাকে অবহেলা করছে\", \"response\": \"অবহেলা খুব কষ্ট দেয়। অনেক সময় ভুল বোঝাবুঝির কারণেও এমন হয়, তুমি কি তাদের সাথে কথা বলে দেখেছ?\"},\n]\n\n# ============================================================\n# 7. END-TO-END PIPELINE\n# ============================================================\n\n# 7.1 Choose strategy (LoRA or Unsloth)\nstrategy = LoRAStrategy(r=4, lora_alpha=8, lora_dropout=0.05)\n# (UnslothStrategy stub exists to satisfy Strategy pattern requirement.)\n\n# 7.2 Initialize fine-tuner\ntuner = LLAMAFineTuner(\n    strategy=strategy,\n    model_name=MODEL_NAME,\n    hf_token=HF_TOKEN,\n    max_seq_len=MAX_SEQ_LEN,\n)\n\n# 7.3 Prepare dataset\nprocessor = DatasetProcessor(tuner.tokenizer, MAX_SEQ_LEN)\ntrain_ds, val_ds, val_raw = processor.prepare(bengali_data, val_size=0.2)\n\n# 7.4 Build Trainer\ntrainer = tuner.get_trainer(train_ds, val_ds)\n\n# 7.5 Train\ntrainer.train()\n\n# 7.6 Evaluation: Perplexity, BLEU, ROUGE\nclear_memory()\nevaluator = Evaluator(\n    model=tuner.model,\n    tokenizer=tuner.tokenizer,\n    processor=processor,\n    trainer=trainer,\n    max_seq_len=MAX_SEQ_LEN,\n)\n\nppl, val_loss = evaluator.compute_perplexity(val_ds)\nprint(f\"\\nValidation loss: {val_loss:.4f}\")\nprint(f\"Perplexity: {ppl:.2f}\")\n\nmetrics_br, inputs, preds, refs = evaluator.compute_bleu_rouge(\n    val_raw,\n    num_samples=None,   # all val examples\n)\n\nprint(\"\\nBLEU:\", metrics_br[\"bleu\"])\nprint(\"ROUGE-1:\", metrics_br[\"rouge1\"])\nprint(\"ROUGE-2:\", metrics_br[\"rouge2\"])\nprint(\"ROUGE-L:\", metrics_br[\"rougeL\"])\n\n# 7.7 Human eval export file\nhuman_eval_path = evaluator.prepare_human_eval_file(\n    inputs=inputs,\n    references=refs,\n    predictions=preds,\n    path=\"human_eval_samples.jsonl\",\n)\nprint(\"Human eval samples saved to:\", human_eval_path)\n\n# 7.8 Logging to SQLite\nlogger = ProjectLogger()\n\n# Get last training loss from Trainer log\ntrain_loss = None\nif trainer.state.log_history:\n    for log in reversed(trainer.state.log_history):\n        if \"loss\" in log:\n            train_loss = log[\"loss\"]\n            break\n\nall_metrics = {\"perplexity\": ppl}\nall_metrics.update(metrics_br)\n\nexperiment_id = logger.log_experiment(\n    model_name=MODEL_NAME,\n    lora_config_dict=tuner.lora_config,\n    train_loss=train_loss,\n    val_loss=val_loss,\n    metrics_dict=all_metrics,\n)\nprint(\"Experiment logged with ID:\", experiment_id)\n\n# Log generated responses for this experiment\nfor inp, pred in zip(inputs, preds):\n    logger.log_response(\n        experiment_id=experiment_id,\n        input_text=inp,\n        response_text=pred,\n    )\n\n# ============================================================\n# 8. SAVE MODEL & TOKENIZER\n# ============================================================\n\ntuner.model.to(\"cpu\")\nclear_memory()\n\nsave_dir = \"bengali-llama31-lora-4bit-final\"\ntuner.model.save_pretrained(save_dir)\ntuner.tokenizer.save_pretrained(save_dir)\n\nprint(\"\\nModel & tokenizer saved to:\", save_dir)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-05T18:44:02.368031Z","iopub.execute_input":"2026-01-05T18:44:02.368409Z","iopub.status.idle":"2026-01-05T18:47:01.780894Z","shell.execute_reply.started":"2026-01-05T18:44:02.368378Z","shell.execute_reply":"2026-01-05T18:47:01.780094Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a08b46439e10497a97ced0da75e07f49"}},"metadata":{}},{"name":"stdout","text":"trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e42b9c05c704a7e8feb9aa2dbcd65f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92d35976c6704e76af98eec47ec6ec0d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12/12 00:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>1.070300</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.899900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/utils/other.py:1394: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-695c06e4-57b036f558c892cf7ab2e7e6;ebc3a46c-9bc2-42a9-b2d0-5c65f77a1901)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4/4 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nValidation loss: 1.0266\nPerplexity: 2.79\n\nBLEU: 0.0\nROUGE-1: 0.0\nROUGE-2: 0.0\nROUGE-L: 0.0\nHuman eval samples saved to: human_eval_samples.jsonl\nExperiment logged with ID: 6\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/utils/other.py:1394: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-695c0725-4451f943745b6ec666f9d6a2;58773059-a4bf-479a-9eb7-c535fdecf47b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nModel & tokenizer saved to: bengali-llama31-lora-4bit-final\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\nMODEL_BASE = \"meta-llama/Llama-3.1-8B-Instruct\"\nLORA_PATH = \"bengali-llama31-lora-4bit-final\"  # your saved folder\nHF_TOKEN = os.environ.get(\"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\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_BASE,\n    token=HF_TOKEN,\n    use_fast=False\n)\ntokenizer.pad_token = tokenizer.eos_token\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_BASE,\n    token=HF_TOKEN,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n)\n\nmodel = PeftModel.from_pretrained(base_model, LORA_PATH)\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T18:47:07.847567Z","iopub.execute_input":"2026-01-05T18:47:07.848224Z","iopub.status.idle":"2026-01-05T18:48:09.197639Z","shell.execute_reply.started":"2026-01-05T18:47:07.848191Z","shell.execute_reply":"2026-01-05T18:48:09.196908Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b589fa49b034ad49b8e5c37cef60597"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128256, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"def test_finetuned_model(\n    model,\n    tokenizer,\n    user_text: str,\n    max_new_tokens: int = 128,\n    temperature: float = 0.7,\n):\n    \"\"\"\n    Simple post-finetuning test function.\n    Call this after training to verify behavior.\n    \"\"\"\n\n    prompt = (\n        \"<|begin_of_text|>\"\n        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n        f\"{user_text}<|eot_id|>\"\n        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    )\n\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=512,\n    ).to(model.device)\n\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=temperature,\n            top_p=0.9,\n            repetition_penalty=1.2,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\n    return response\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T18:48:42.705453Z","iopub.execute_input":"2026-01-05T18:48:42.705826Z","iopub.status.idle":"2026-01-05T18:48:42.711917Z","shell.execute_reply.started":"2026-01-05T18:48:42.705794Z","shell.execute_reply":"2026-01-05T18:48:42.711228Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"test_cases = [\n    \"আমার খুব একা লাগছে\",\n    \"আমি খুব হতাশ বোধ করছি\",\n    \"কেউ আমাকে বোঝে না\",\n    \"ভবিষ্যৎ নিয়ে আমি খুব চিন্তিত\",\n    \"আমি মনে হয় জীবনে কিছুই করতে পারব না\",\n]\n\nprint(\"\\n===== MODEL TEST RESULTS =====\\n\")\n\nfor i, text in enumerate(test_cases, 1):\n    output = test_finetuned_model(model, tokenizer, text)\n    print(f\"[{i}] USER: {text}\")\n    print(f\"[{i}] ASSISTANT:\\n{output}\")\n    print(\"-\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T18:48:46.157062Z","iopub.execute_input":"2026-01-05T18:48:46.157617Z","iopub.status.idle":"2026-01-05T18:49:57.343028Z","shell.execute_reply.started":"2026-01-05T18:48:46.157588Z","shell.execute_reply":"2026-01-05T18:49:57.342236Z"}},"outputs":[{"name":"stdout","text":"\n===== MODEL TEST RESULTS =====\n\n[1] USER: আমার খুব একা লাগছে\n[1] ASSISTANT:\nuser\n\nআমার খুব একা লাগছেassistant\n\nতোর সাথে যোগাযোগ করি, হ্যাংগিন অনুভৃতি পড়ে।\n------------------------------------------------------------\n[2] USER: আমি খুব হতাশ বোধ করছি\n[2] ASSISTANT:\nuser\n\nআমি খুব হতাশ বোধ করছিassistant\n\nসে যা নিজেই প্রণয়, এটি অন্যের চেষ্টাও। ভালো-মন্দ ফুরিয়ে গেলে মাথায় ঠিকমতো ঢেউ ডুমবে না।\n\n*   ঘৃণ্য না হতে\n------------------------------------------------------------\n[3] USER: কেউ আমাকে বোঝে না\n[3] ASSISTANT:\nuser\n\nকেউ আমাকে বোঝে নাassistant\n\n\"তুচ্ছ-হয়, সন্দেহ।\"\n\n* যারা এর ভিডিয়ো দেখেছে, তারা জানে প্রশ্নটি কীই থাকল।\n \n \"অফ... গুড!\"\n------------------------------------------------------------\n[4] USER: ভবিষ্যৎ নিয়ে আমি খুব চিন্তিত\n[4] ASSISTANT:\nuser\n\nভবিষ্যৎ নিয়ে আমি খুব চিন্তিতassistant\n\nএরা সাধারণই।  প্রথমে, হৃদয়ে উন্মাদ করো না, জীবনের অংশ-অংশে ছোড়া ফেলো. সঙ্গে সঙ্গে এটাও বলো - 'আমি কার\n------------------------------------------------------------\n[5] USER: আমি মনে হয় জীবনে কিছুই করতে পারব না\n[5] ASSISTANT:\nuser\n\nআমি মনে হয় জীবনে কিছুই করতে পারব নাassistant\n\nস্যাড, এটি খোলা-ওঠাশের দিচ্ছ। \n\nধরুন ভবিষ্যতের অনেক ঘটনা থাকবে; ফৌজদারী গ্রেফতার, সংঘবিঞ্চন, দাঙ্গা, ঝগড়\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ===============================\n#  Test multiple inputs nicely\n# ===============================\n\ntest_cases = [\n    \"আমার খুব একা লাগছে\",\n    \"আমি খুব হতাশ বোধ করছি\",\n    \"কেউ আমাকে বোঝে না\",\n    \"ভবিষ্যৎ নিয়ে আমি খুব চিন্তিত\",\n    \"আমি মনে হয় জীবনে কিছুই করতে পারব না\",\n]\n\ndef test_finetuned_model(model, tokenizer, user_input):\n    \"\"\"\n    Wrapper function for generating responses with nice prompt formatting.\n    \"\"\"\n    prompt = (\n        \"<|begin_of_text|>\"\n        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n        f\"{user_input}<|eot_id|>\"\n        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    )\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            repetition_penalty=1.2,\n            no_repeat_ngram_size=3,\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n    \n    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    # Extract only assistant's reply\n    response = full_text.split(\"assistant\")[-1].strip() if \"assistant\" in full_text else full_text\n    return response\n\nprint(\"\\n===== MODEL TEST RESULTS =====\\n\")\n\nfor i, text in enumerate(test_cases, 1):\n    output = test_finetuned_model(model, tokenizer, text)\n    print(f\"[{i}] USER:      {text}\")\n    print(f\"[{i}] ASSISTANT: {output}\\n\")\n    print(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T18:50:48.029154Z","iopub.execute_input":"2026-01-05T18:50:48.029472Z","execution_failed":"2026-01-05T19:33:31.943Z"}},"outputs":[{"name":"stdout","text":"\n===== MODEL TEST RESULTS =====\n\n[1] USER:      আমার খুব একা লাগছে\n[1] ASSISTANT: নির্ভয়, তোমরা সহায়তা পেতে চিন্তিত থাকো না। ধরুন শুরূতি দিই:\n\n*   ডাওন উঠলে, সন্ধ্যার (4-5:00 pm) ৩/৪ ঘন্টা অফিসে থেমনে যাও।\n*   6:30 টা, ঘৃ�\n\n================================================================================\n[2] USER:      আমি খুব হতাশ বোধ করছি\n[2] ASSISTANT: নিজের চ্যালেঞ্জগু‌লো পা‌র-সফল ভাবে অতিক্র‌‍‌‌‌‍া‌ন ‌থে‌কে‌‌‍ এই নেওয়া উচি‌ত। দৃ‌ষ্টিভঙ্গি পরিবর‌্তন ক‌রা, নিজস্ব আত্মবিশ্বাস বাড�\n\n================================================================================\n[3] USER:      কেউ আমাকে বোঝে না\n[3] ASSISTANT: \"এরপর, যদি স্টিফেন হতে চাওছিল অন্য জীভার গঠন, শুধু এই দৃষ্টান্তেই।  'খাও,' বলত'—- সংঘণাঙ্কাঞ্জ কৌড়া থেক নিবৈ গুৎ. ঐ সামজে �\n\n================================================================================\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ===============================\n#  TEST MULTIPLE INPUTS USING STEP 12 PROMPT\n# ===============================\n\ntest_cases = [\n    \"আমার খুব একা লাগছে\",\n    \"আমি খুব হতাশ বোধ করছি\",\n    \"কেউ আমাকে বোঝে না\",\n    \"ভবিষ্যৎ নিয়ে আমি খুব চিন্তিত\",\n    \"আমি মনে হয় জীবনে কিছুই করতে পারব না\",\n]\n\ndef test_finetuned_model(model, tokenizer, user_input):\n    \"\"\"\n    Generate response using STEP 12 LLaMA 3.1 prompt format.\n    \"\"\"\n    prompt = (\n        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n        f\"{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    )\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            repetition_penalty=1.2,\n            no_repeat_ngram_size=3,\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n    \n    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    # Extract only assistant's reply\n    response = full_text.split(\"assistant\")[-1].strip() if \"assistant\" in full_text else full_text\n    return response\n\nprint(\"\\n===== MODEL TEST RESULTS =====\\n\")\n\nfor i, text in enumerate(test_cases, 1):\n    output = test_finetuned_model(model, tokenizer, text)\n    print(f\"[{i}] USER:      {text}\")\n    print(f\"[{i}] ASSISTANT: {output}\\n\")\n    print(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T18:54:44.480156Z","iopub.execute_input":"2026-01-05T18:54:44.48046Z","execution_failed":"2026-01-05T19:33:31.944Z"}},"outputs":[{"name":"stdout","text":"\n===== MODEL TEST RESULTS =====\n\n[1] USER:      আমার খুব একা লাগছে\n[1] ASSISTANT: নিজের সাথে যোগাযু্হ্ধ, নিরাপদ ও শান্ত। \n- 1.অভ্যাস: ঘরে চলুন টেলিফোন ডাইল বন্ধ করু'ন। \n\n2. উঠো ঝুয়ে দিন।\n\n3. বাইরেরেকটের্ণে ঢু:ক।\n4. প্�\n\n================================================================================\n","output_type":"stream"}],"execution_count":null}]}